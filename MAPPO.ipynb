{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "PRsA89LOIoxy"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!git clone https://github.com/cuongtv312/marl-delivery.git\n",
        "%cd marl-delivery\n",
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "1ZoLo1XMIs4y"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install stable-baselines3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "WwmghuewItvv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "08481d168cdc444a8c744355f7b701da",
            "ee50af5827b040aa8b6b50fe4a2dbd38",
            "2ea2c4b272514f6abd739fec1112510e",
            "5273763364454464b69c6f799798507c",
            "a7cda6e563154ccbad32d66cfc55aa5a",
            "e3b069e321a74878a2ff033e4d790076",
            "1bcca8c264cf47ca8338b76ddfcbe18e",
            "a0f175f76e9b4f82a821337b1564f527",
            "3b34aa7b622c4bec99a3a9667e740c9d",
            "b9545103f47b473da4719bf620301ba7",
            "38ce772bd6ec49bd9f2c84153b901f9d"
          ]
        },
        "outputId": "77365669-9cdd-49c3-ac80-7a7394196bae"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/2000 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "08481d168cdc444a8c744355f7b701da"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 0: Total Reward = -16.879999202489852, Cumulative Reward = 0.0, Average Reward = -16.879999202489852\n",
            "Iteration 1: Total Reward = -15.569999206066132, Cumulative Reward = 0.0, Average Reward = -16.224999204277992\n",
            "Iteration 2: Total Reward = -14.049999192357063, Cumulative Reward = 0.0, Average Reward = -15.49999920030435\n",
            "Iteration 3: Total Reward = -14.439999336004258, Cumulative Reward = 0.0, Average Reward = -15.234999234229326\n",
            "Iteration 4: Total Reward = -15.979999211430549, Cumulative Reward = 0.0, Average Reward = -15.38399922966957\n",
            "Iteration 5: Total Reward = -14.859999188780785, Cumulative Reward = 0.0, Average Reward = -15.29666588952144\n",
            "Iteration 6: Total Reward = -16.589999094605446, Cumulative Reward = 0.0, Average Reward = -15.481427775962013\n",
            "Iteration 7: Total Reward = -6.309999829530716, Cumulative Reward = 0.0, Average Reward = -14.3349992826581\n",
            "Iteration 8: Total Reward = -16.349999204277992, Cumulative Reward = 0.0, Average Reward = -14.558888162838088\n",
            "Iteration 9: Total Reward = -13.979999172687531, Cumulative Reward = 0.0, Average Reward = -14.500999263823033\n",
            "Iteration 10: Total Reward = -15.469999185204506, Cumulative Reward = 0.0, Average Reward = -14.589090165766804\n",
            "Iteration 11: Total Reward = -14.459999093413353, Cumulative Reward = 0.0, Average Reward = -14.578332576404016\n",
            "Iteration 12: Total Reward = -13.13999937772751, Cumulative Reward = 0.0, Average Reward = -14.467691561121208\n",
            "Iteration 13: Total Reward = -16.53999916315079, Cumulative Reward = 0.0, Average Reward = -14.61571353269475\n",
            "Iteration 14: Total Reward = -15.229999172687531, Cumulative Reward = 0.0, Average Reward = -14.656665908694269\n",
            "Iteration 15: Total Reward = -6.079999393224716, Cumulative Reward = 0.0, Average Reward = -14.120624251477421\n",
            "Iteration 16: Total Reward = -17.559999203681947, Cumulative Reward = 0.0, Average Reward = -14.32294042513651\n",
            "Iteration 17: Total Reward = -16.679999175667763, Cumulative Reward = 0.0, Average Reward = -14.453888133499358\n",
            "Iteration 18: Total Reward = -15.499999248981476, Cumulative Reward = 0.0, Average Reward = -14.508946613261575\n",
            "Iteration 19: Total Reward = -7.319999203085899, Cumulative Reward = 0.0, Average Reward = -14.14949924275279\n",
            "Iteration 20: Total Reward = -15.479999044537545, Cumulative Reward = 0.0, Average Reward = -14.21285637617111\n",
            "Iteration 21: Total Reward = -15.249999186396598, Cumulative Reward = 0.0, Average Reward = -14.25999923118136\n",
            "Iteration 22: Total Reward = -16.60999913215637, Cumulative Reward = 0.0, Average Reward = -14.362173139919404\n",
            "Iteration 23: Total Reward = -16.04999920129776, Cumulative Reward = 0.0, Average Reward = -14.43249922581017\n",
            "Iteration 24: Total Reward = -14.749999198317528, Cumulative Reward = 0.0, Average Reward = -14.445199224710466\n",
            "Iteration 25: Total Reward = -15.11999919116497, Cumulative Reward = 0.0, Average Reward = -14.471153069574102\n",
            "Iteration 26: Total Reward = -16.13999911546707, Cumulative Reward = 0.0, Average Reward = -14.532962182384951\n",
            "Iteration 27: Total Reward = -13.929999241232872, Cumulative Reward = 0.0, Average Reward = -14.511427791629519\n",
            "Iteration 28: Total Reward = -14.929999125003814, Cumulative Reward = 0.0, Average Reward = -14.525861285883805\n",
            "Iteration 29: Total Reward = -3.2199993282556534, Cumulative Reward = 0.0, Average Reward = -14.148999220629532\n",
            "Iteration 30: Total Reward = -14.32999933063984, Cumulative Reward = 0.0, Average Reward = -14.154837933855672\n",
            "Iteration 31: Total Reward = -16.2299992531538, Cumulative Reward = 0.0, Average Reward = -14.21968672508374\n",
            "Iteration 32: Total Reward = -16.479999095201492, Cumulative Reward = 0.0, Average Reward = -14.288181039329732\n",
            "Iteration 33: Total Reward = -15.809999132156372, Cumulative Reward = 0.0, Average Reward = -14.332940395001103\n",
            "Iteration 34: Total Reward = -16.08999916613102, Cumulative Reward = 0.0, Average Reward = -14.383142074176243\n",
            "Iteration 35: Total Reward = -15.519999235868454, Cumulative Reward = 0.0, Average Reward = -14.414721439778804\n",
            "Iteration 36: Total Reward = -15.589999145269394, Cumulative Reward = 0.0, Average Reward = -14.44648570208936\n",
            "Iteration 37: Total Reward = -14.449999085068702, Cumulative Reward = 0.0, Average Reward = -14.446578159536186\n",
            "Iteration 38: Total Reward = -15.389999124407769, Cumulative Reward = 0.0, Average Reward = -14.470768440686738\n",
            "Iteration 39: Total Reward = -16.209999153017996, Cumulative Reward = 0.0, Average Reward = -14.51424920849502\n",
            "Iteration 40: Total Reward = -16.109999254345894, Cumulative Reward = 0.0, Average Reward = -14.55316994132065\n",
            "Iteration 41: Total Reward = -15.789999270439148, Cumulative Reward = 0.0, Average Reward = -14.582618258680615\n",
            "Iteration 42: Total Reward = -13.069999212026596, Cumulative Reward = 0.0, Average Reward = -14.547441071549125\n",
            "Iteration 43: Total Reward = -14.33999896645546, Cumulative Reward = 0.0, Average Reward = -14.542726478251542\n",
            "Iteration 44: Total Reward = -15.079999116063117, Cumulative Reward = 0.0, Average Reward = -14.55466587020291\n",
            "Iteration 45: Total Reward = -16.109999218583106, Cumulative Reward = 0.0, Average Reward = -14.588477464732915\n",
            "Iteration 46: Total Reward = -12.489999055862427, Cumulative Reward = 0.0, Average Reward = -14.543828987948437\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-37340e9405ee>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 485\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-23-37340e9405ee>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    480\u001b[0m     \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMAPPO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-23-37340e9405ee>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    418\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Reward/Average'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavg_reward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 420\u001b[0;31m             \u001b[0mpolicy_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentropy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshaped_rews\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    421\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Reward'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_reward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Policy_Loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-23-37340e9405ee>\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, obs, acts, lps, vals, shaped_rews, masks)\u001b[0m\n\u001b[1;32m    364\u001b[0m                     \u001b[0mMAX_GRAD_NORM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m                 )\n\u001b[0;32m--> 366\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    367\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m                 \u001b[0;31m# — Cộng dồn để lấy trung bình cuối cùng —\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m                     \u001b[0mopt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopt_ref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m                     \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_opt_called\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m  \u001b[0;31m# type: ignore[union-attr]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m                 \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped_by_lr_sched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m                             )\n\u001b[1;32m    492\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m_use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"differentiable\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    242\u001b[0m             )\n\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m             adam(\n\u001b[0m\u001b[1;32m    245\u001b[0m                 \u001b[0mparams_with_grad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m                 \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mmaybe_fallback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    152\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mdisabled_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_fallback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    874\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_single_tensor_adam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 876\u001b[0;31m     func(\n\u001b[0m\u001b[1;32m    877\u001b[0m         \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    878\u001b[0m         \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    474\u001b[0m                 \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_exp_avg_sqs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction2_sqrt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 476\u001b[0;31m                 \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction2_sqrt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    478\u001b[0m             \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_avg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdenom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import argparse\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import time\n",
        "from collections import deque\n",
        "from tqdm.auto import tqdm\n",
        "from env import Environment\n",
        "from greedyagent import GreedyAgents\n",
        "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
        "\n",
        "# Hyperparameters\n",
        "GAMMA = 0.99\n",
        "LAMBDA = 0.95\n",
        "CLIP_EPS = 0.3\n",
        "LR_INITIAL = 2e-4\n",
        "LR_PPO = 1e-4\n",
        "PPO_EPOCHS = 16\n",
        "MINIBATCH_SIZE = 256\n",
        "HORIZON = 512\n",
        "ENTROPY_COEF_HIGH = 1.0\n",
        "ENTROPY_COEF_LOW = 0.01\n",
        "VALUE_COEF = 0.5\n",
        "MAX_GRAD_NORM = 0.5\n",
        "MAX_EPISODES = 100\n",
        "BC_EPOCHS = 10\n",
        "BC_BATCH_SIZE = 128\n",
        "UPDATES = 2000\n",
        "\n",
        "torch.set_num_threads(1)\n",
        "\n",
        "def set_seed(seed):\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "\n",
        "class RunningMeanStd:\n",
        "    def __init__(self, epsilon=1e-4, shape=()):\n",
        "        self.mean = np.zeros(shape, 'float64')\n",
        "        self.var = np.ones(shape, 'float64')\n",
        "        self.count = epsilon\n",
        "\n",
        "    def update(self, x):\n",
        "        x = np.array(x)\n",
        "        batch_mean = np.mean(x, axis=0)\n",
        "        batch_var = np.var(x, axis=0)\n",
        "        batch_count = x.shape[0]\n",
        "        delta = batch_mean - self.mean\n",
        "        tot_count = self.count + batch_count\n",
        "        m_a = self.var * self.count\n",
        "        m_b = batch_var * batch_count\n",
        "        M2 = m_a + m_b + np.square(delta) * self.count * batch_count / tot_count\n",
        "        self.mean = self.mean + delta * batch_count / tot_count\n",
        "        self.var = M2 / tot_count\n",
        "        self.count = tot_count\n",
        "\n",
        "    def normalize(self, x):\n",
        "        return (x - self.mean) / np.sqrt(self.var + 1e-8)\n",
        "\n",
        "class StateEncoder(nn.Module):\n",
        "    def __init__(self, n_agents, embed_dim=256, d_model=64, nhead=4, num_layers=2):\n",
        "        super().__init__()\n",
        "        self.n_agents = n_agents\n",
        "        self.cnn = nn.Sequential(\n",
        "            nn.Conv2d(6, 64, kernel_size=3, padding=1), nn.ReLU(),\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1), nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Conv2d(128, 256, kernel_size=3, padding=1), nn.ReLU(),\n",
        "            nn.AdaptiveAvgPool2d((1,1)), nn.Flatten()\n",
        "        )\n",
        "        self.agent_embed = nn.Linear(6, d_model)\n",
        "        self.pos_embed = nn.Embedding(n_agents, d_model)\n",
        "        encoder_layer = TransformerEncoderLayer(d_model, nhead, batch_first=True)\n",
        "        self.transformer = TransformerEncoder(encoder_layer, num_layers)\n",
        "        total_in = 256 + d_model + 1\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(total_in, embed_dim), nn.ReLU(),\n",
        "            nn.Linear(embed_dim, embed_dim), nn.ReLU()\n",
        "        )\n",
        "\n",
        "    def forward(self, spatial, non_spatial):\n",
        "        B = spatial.size(0)\n",
        "        cnn_out = self.cnn(spatial)\n",
        "        pa = non_spatial[:, :self.n_agents*6].view(B, self.n_agents, 6)\n",
        "        em = self.agent_embed(pa)\n",
        "        idx = torch.arange(self.n_agents, device=spatial.device).unsqueeze(0).repeat(B,1)\n",
        "        pe = self.pos_embed(idx)\n",
        "        tr_in = em + pe\n",
        "        tr_out = self.transformer(tr_in).mean(dim=1)\n",
        "        t = non_spatial[:, -1].unsqueeze(1)\n",
        "        x = torch.cat([cnn_out, tr_out, t], dim=1)\n",
        "        return self.mlp(x)\n",
        "\n",
        "class ActorCritic(nn.Module):\n",
        "    def __init__(self, embed_dim, n_agents, action_per_agent):\n",
        "        super().__init__()\n",
        "        self.n_agents = n_agents\n",
        "        self.ap = action_per_agent\n",
        "        self.actor = nn.Linear(embed_dim, n_agents * action_per_agent)\n",
        "        self.critic = nn.Linear(embed_dim, n_agents)\n",
        "\n",
        "    def forward(self, h):\n",
        "        B = h.size(0)\n",
        "        logits = self.actor(h).view(B, self.n_agents, self.ap)\n",
        "        dists = [torch.distributions.Categorical(logits=logits[:,i,:]) for i in range(self.n_agents)]\n",
        "        val = self.critic(h)\n",
        "        return dists, val\n",
        "\n",
        "class MAPPO:\n",
        "    def __init__(self, env, device, max_steps, args):\n",
        "        self.env = env\n",
        "        self.device = device\n",
        "        self.max_steps = max_steps\n",
        "        self.args = args\n",
        "        n = env.n_robots\n",
        "        ap = 5 * 3\n",
        "        if min(env.n_rows, env.n_cols) < 2:\n",
        "            self.encoder = StateEncoder(n_agents=n, embed_dim=256, d_model=64, nhead=4, num_layers=2).to(device)\n",
        "            self.encoder.cnn = nn.Sequential(\n",
        "                nn.Conv2d(6, 64, kernel_size=3, padding=1), nn.ReLU(),\n",
        "                nn.Conv2d(64, 128, kernel_size=3, padding=1), nn.ReLU(),\n",
        "                nn.Conv2d(128, 256, kernel_size=3, padding=1), nn.ReLU(),\n",
        "                nn.AdaptiveAvgPool2d((1,1)), nn.Flatten()\n",
        "            )\n",
        "        else:\n",
        "            self.encoder = StateEncoder(n_agents=n).to(device)\n",
        "        self.ac = ActorCritic(embed_dim=256, n_agents=n, action_per_agent=ap).to(device)\n",
        "        self.optimizer = optim.Adam(\n",
        "            list(self.encoder.parameters()) + list(self.ac.parameters()), lr=LR_INITIAL)\n",
        "        self.scheduler = optim.lr_scheduler.StepLR(self.optimizer, step_size=100, gamma=0.9)\n",
        "        self.writer = SummaryWriter()\n",
        "        self.ep_rewards = deque(maxlen=MAX_EPISODES)\n",
        "        self.best = -np.inf\n",
        "        self.total_ts = 0\n",
        "        self.norm = RunningMeanStd(shape=(n,))\n",
        "        self.ent_coef = ENTROPY_COEF_HIGH\n",
        "\n",
        "    def create_spatial_grid(self, state, pos, nearest_targets):\n",
        "        n_rows, n_cols = self.env.n_rows, self.env.n_cols\n",
        "        grid = torch.zeros((6, n_rows, n_cols), dtype=torch.float32, device=self.device)\n",
        "        grid[0] = torch.tensor(state['map'], dtype=torch.float32, device=self.device)\n",
        "        for robot in state['robots']:\n",
        "            r, c = int(robot[0]), int(robot[1])\n",
        "            if 0 <= r < n_rows and 0 <= c < n_cols:\n",
        "                grid[1, r, c] = 1.0\n",
        "        for robot in state['robots']:\n",
        "            r, c = int(robot[0]), int(robot[1])\n",
        "            if 0 <= r < n_rows and 0 <= c < n_cols and robot[2] > -1:\n",
        "                grid[2, r, c] = 1.0\n",
        "        waiting = {(int(p[1]), int(p[2])) for p in state['packages'] if p[5] == -1}\n",
        "        for r, c in waiting:\n",
        "            if 0 <= r < n_rows and 0 <= c < n_cols:\n",
        "                grid[3, r, c] = 1.0\n",
        "        ends = {(int(p[3]), int(p[4])) for p in state['packages']}\n",
        "        for r, c in ends:\n",
        "            if 0 <= r < n_rows and 0 <= c < n_cols:\n",
        "                grid[4, r, c] = 1.0\n",
        "        for target in nearest_targets:\n",
        "            if target is not None:\n",
        "                r, c = target\n",
        "                if 0 <= r < n_rows and 0 <= c < n_cols:\n",
        "                    grid[5, r, c] = 1.0\n",
        "        return grid\n",
        "\n",
        "    def get_distance(self, r1, c1, r2, c2):\n",
        "        return abs(r1 - r2) + abs(c1 - c2)\n",
        "\n",
        "    def compute_potential(self, state):\n",
        "        robots = np.array(state['robots'])\n",
        "        pkg_list = state['packages']\n",
        "        potential = 0.0\n",
        "        for i in range(self.env.n_robots):\n",
        "            r_robot, c_robot = int(robots[i][0]), int(robots[i][1])\n",
        "            carrying = robots[i][2] > -1\n",
        "            if carrying:\n",
        "                pkg_id = int(robots[i][2])\n",
        "                for p in pkg_list:\n",
        "                    if p[0] == pkg_id and p[5] != -1:\n",
        "                        r_target, c_target = int(p[3]), int(p[4])\n",
        "                        potential -= self.get_distance(r_robot, c_robot, r_target, c_target)\n",
        "                        break\n",
        "            else:\n",
        "                waiting_packages = [p for p in pkg_list if p[5] == -1]\n",
        "                if waiting_packages:\n",
        "                    dists = [self.get_distance(r_robot, c_robot, int(p[1]), int(p[2])) for p in waiting_packages]\n",
        "                    potential -= min(dists)\n",
        "        return potential\n",
        "\n",
        "    def convert_state(self, state, t):\n",
        "        robots = np.array(state['robots'], dtype=np.float32)\n",
        "        carry_status = (robots[:, 2] > -1).astype(np.float32)\n",
        "        pkg_list = state['packages']\n",
        "        pos = robots[:, :2]\n",
        "        n_agents = self.env.n_robots\n",
        "        max_rows_cols = max(self.env.n_rows, self.env.n_cols)\n",
        "        normalized_pos = pos / max_rows_cols\n",
        "        normalized_nearest_start = np.zeros((n_agents, 2), dtype=np.float32)\n",
        "        normalized_nearest_end = np.zeros((n_agents, 2), dtype=np.float32)\n",
        "        normalized_nearest_dead = np.zeros((n_agents, 1), dtype=np.float32)\n",
        "        nearest_targets = []\n",
        "\n",
        "        for i in range(n_agents):\n",
        "            r_robot = int(robots[i][0])\n",
        "            c_robot = int(robots[i][1])\n",
        "            if carry_status[i] == 0:\n",
        "                waiting_packages = [p for p in pkg_list if p[5] == -1]\n",
        "                if waiting_packages:\n",
        "                    dists = [self.get_distance(r_robot, c_robot, int(p[1]), int(p[2])) for p in waiting_packages]\n",
        "                    idx_min = np.argmin(dists)\n",
        "                    nearest_pkg = waiting_packages[idx_min]\n",
        "                    normalized_nearest_start[i] = [nearest_pkg[1] / max_rows_cols, nearest_pkg[2] / max_rows_cols]\n",
        "                    normalized_nearest_end[i] = [nearest_pkg[3] / max_rows_cols, nearest_pkg[4] / max_rows_cols]\n",
        "                    normalized_nearest_dead[i] = [nearest_pkg[6] / self.max_steps]\n",
        "                    nearest_targets.append((int(nearest_pkg[1]), int(nearest_pkg[2])))\n",
        "                else:\n",
        "                    normalized_nearest_start[i] = [0, 0]\n",
        "                    normalized_nearest_end[i] = [0, 0]\n",
        "                    normalized_nearest_dead[i] = [0]\n",
        "                    nearest_targets.append(None)\n",
        "            else:\n",
        "                pkg_id = int(robots[i][2])\n",
        "                found = False\n",
        "                for p in pkg_list:\n",
        "                    if p[0] == pkg_id and p[5] != -1:\n",
        "                        normalized_nearest_start[i] = [p[1] / max_rows_cols, p[2] / max_rows_cols]\n",
        "                        normalized_nearest_end[i] = [p[3] / max_rows_cols, p[4] / max_rows_cols]\n",
        "                        normalized_nearest_dead[i] = [p[6] / self.max_steps]\n",
        "                        nearest_targets.append((int(p[3]), int(p[4])))\n",
        "                        found = True\n",
        "                        break\n",
        "                if not found:\n",
        "                    normalized_nearest_start[i] = [0, 0]\n",
        "                    normalized_nearest_end[i] = [0, 0]\n",
        "                    normalized_nearest_dead[i] = [0]\n",
        "                    nearest_targets.append(None)\n",
        "\n",
        "        t_norm = float(t) / self.max_steps\n",
        "        non_spatial = np.concatenate([\n",
        "            carry_status.flatten(),\n",
        "            normalized_nearest_start.flatten(),\n",
        "            normalized_nearest_end.flatten(),\n",
        "            normalized_nearest_dead.flatten(),\n",
        "            [t_norm]\n",
        "        ], axis=0).astype(np.float32)\n",
        "        spatial_grid = self.create_spatial_grid(state, pos, nearest_targets)\n",
        "        return spatial_grid.clone().detach().to(self.device).unsqueeze(0), torch.tensor(non_spatial, device=self.device).unsqueeze(0)\n",
        "    def rollout(self):\n",
        "        obs, acts, lps, vals, rews, shaped_rews, masks = [], [], [], [], [], [], []\n",
        "        state = self.env.reset()\n",
        "        for t in range(HORIZON):\n",
        "            spatial, non_spatial = self.convert_state(state, t)\n",
        "            with torch.no_grad():\n",
        "                h = self.encoder(spatial, non_spatial)\n",
        "                dists, val = self.ac(h)\n",
        "            actions = [dist.sample() for dist in dists]\n",
        "            log_probs = [dists[j].log_prob(actions[j]).detach() for j in range(self.ac.n_agents)]\n",
        "            action_array = torch.stack(actions, dim=1).cpu().numpy()\n",
        "            Phi_current = self.compute_potential(state)\n",
        "            # Sử dụng:\n",
        "            integer_actions = [action.item() for action in actions]  # Chuyển tensor thành số nguyên\n",
        "            env_actions = [( ['S','L','R','U','D'][a//3], str(a%3) ) for a in integer_actions]  # Chuyển sang tuple\n",
        "            state, reward_list, done, _ = self.env.step(env_actions)  # Truyền danh sách tuple\n",
        "            # Nếu env.step trả về một số, chuyển thành list lặp lại cho mỗi agent\n",
        "            if isinstance(reward_list, (float, int)):\n",
        "                reward_list = [reward_list] * self.env.n_robots\n",
        "            # Nếu trả về numpy array, chuyển thành list Python\n",
        "            elif isinstance(reward_list, np.ndarray):\n",
        "                reward_list = reward_list.tolist()\n",
        "            Phi_next = self.compute_potential(state)\n",
        "            shaping_term = (GAMMA * Phi_next - Phi_current) / self.env.n_robots\n",
        "            shaped_reward_list = [r + shaping_term for r in reward_list]\n",
        "\n",
        "\n",
        "            mask = 0.0 if done else 1.0\n",
        "            obs.append((spatial, non_spatial))\n",
        "            acts.append(torch.stack(actions))\n",
        "            lps.append(torch.stack(log_probs))\n",
        "            vals.append(val.squeeze(0))\n",
        "            rews.append(torch.tensor(reward_list, device=self.device, dtype=torch.float32))\n",
        "            shaped_rews.append(torch.tensor(shaped_reward_list, device=self.device, dtype=torch.float32))\n",
        "            masks.append(mask)\n",
        "            if done:\n",
        "                state = self.env.reset()\n",
        "        spatial, non_spatial = self.convert_state(state, HORIZON)\n",
        "        with torch.no_grad():\n",
        "            h = self.encoder(spatial, non_spatial)\n",
        "            _, val = self.ac(h)\n",
        "        vals.append(val.squeeze(0))\n",
        "        return obs, acts, lps, vals, shaped_rews, masks, rews\n",
        "\n",
        "    def compute_gae(self, rewards, values, masks):\n",
        "        returns = []\n",
        "        gae = torch.zeros(self.ac.n_agents, device=self.device)\n",
        "        for t in reversed(range(HORIZON)):\n",
        "            delta = rewards[t] + GAMMA * values[t + 1] * masks[t] - values[t]\n",
        "            gae = delta + GAMMA * LAMBDA * masks[t] * gae\n",
        "            returns.insert(0, gae + values[t])\n",
        "        returns = torch.stack(returns)\n",
        "        advs = returns - torch.stack(values[:-1])\n",
        "        advs = (advs - advs.mean()) / (advs.std() + 1e-8)\n",
        "        return returns, advs\n",
        "\n",
        "    def update(self, obs, acts, lps, vals, shaped_rews, masks):\n",
        "        # 1. Tính returns và advantages qua GAE\n",
        "        returns, advs = self.compute_gae(shaped_rews, vals, masks)\n",
        "        self.norm.update(advs)\n",
        "\n",
        "        total_policy_loss = 0.0\n",
        "        total_value_loss  = 0.0\n",
        "        total_entropy     = 0.0\n",
        "\n",
        "        # Tạo permutation để sample minibatches\n",
        "        indices = torch.randperm(HORIZON, device=self.device)\n",
        "\n",
        "        for _ in range(PPO_EPOCHS):\n",
        "            for start in range(0, HORIZON, MINIBATCH_SIZE):\n",
        "                end       = start + MINIBATCH_SIZE\n",
        "                batch_idx = indices[start:end]\n",
        "\n",
        "                # — Chuẩn bị batch tensors —\n",
        "                spatial_batch     = torch.cat([obs[t][0] for t in batch_idx], dim=0)  # [B', C, H, W]\n",
        "                non_spatial_batch = torch.cat([obs[t][1] for t in batch_idx], dim=0)  # [B', D]\n",
        "                act_batch         = torch.stack([acts[t] for t in batch_idx], dim=0).squeeze(-1)  # [B', n_agents]\n",
        "                old_lp_batch      = torch.stack([lps[t] for t in batch_idx], dim=0).squeeze(-1)  # [B', n_agents]\n",
        "                return_batch      = returns[batch_idx]    # [B', n_agents]\n",
        "                adv_batch         = advs[batch_idx]       # [B', n_agents]\n",
        "\n",
        "                # — Forward qua network —\n",
        "                h     = self.encoder(spatial_batch, non_spatial_batch)  # [B', embed]\n",
        "                dists, vals_pred = self.ac(h)                           # vals_pred: [B', n_agents]\n",
        "\n",
        "                # Lấy logits thô và compute log_probs thủ công\n",
        "                logits = self.ac.actor(h).view(-1, self.ac.n_agents, self.ac.ap)  # [B', n_agents, ap]\n",
        "                logp   = F.log_softmax(logits, dim=-1)                           # [B', n_agents, ap]\n",
        "\n",
        "                # Sử dụng advanced indexing\n",
        "                B_, N = logp.size(0), logp.size(1)\n",
        "                b_idx = torch.arange(B_, device=self.device).view(-1, 1).expand(-1, N)\n",
        "                a_idx = torch.arange(N, device=self.device).view(1, -1).expand(B_, -1)\n",
        "                new_lp = logp[b_idx, a_idx, act_batch]\n",
        "\n",
        "                # entropy: trung bình entropy của từng agent\n",
        "                entropy = (-(logp * logp.exp()).sum(dim=-1)).mean()\n",
        "\n",
        "                # — PPO surrogate loss —\n",
        "                ratios = torch.exp(new_lp - old_lp_batch)  # [B', n_agents]\n",
        "                surr1  = ratios * adv_batch\n",
        "                surr2  = torch.clamp(ratios, 1 - CLIP_EPS, 1 + CLIP_EPS) * adv_batch\n",
        "                policy_loss = -torch.min(surr1, surr2).mean()\n",
        "\n",
        "                # — Value loss —\n",
        "                value_loss = F.mse_loss(vals_pred, return_batch)\n",
        "\n",
        "                # — Tổng loss —\n",
        "                loss = policy_loss + VALUE_COEF * value_loss - self.ent_coef * entropy\n",
        "\n",
        "                # — Backprop & optimize —\n",
        "                self.optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(\n",
        "                    list(self.encoder.parameters()) + list(self.ac.parameters()),\n",
        "                    MAX_GRAD_NORM\n",
        "                )\n",
        "                self.optimizer.step()\n",
        "\n",
        "                # — Cộng dồn để lấy trung bình cuối cùng —\n",
        "                total_policy_loss += policy_loss.item()\n",
        "                total_value_loss  += value_loss.item()\n",
        "                total_entropy     += entropy.item()\n",
        "\n",
        "        num_updates = PPO_EPOCHS * (HORIZON // MINIBATCH_SIZE)\n",
        "        return (\n",
        "            total_policy_loss / num_updates,\n",
        "            total_value_loss  / num_updates,\n",
        "            total_entropy     / num_updates\n",
        "        )\n",
        "\n",
        "\n",
        "    def train(self):\n",
        "        if self.args.bc:\n",
        "            expert = GreedyAgents(self.env, self.device)\n",
        "            for epoch in range(BC_EPOCHS):\n",
        "                loss_total = 0.0\n",
        "                for _ in range(BC_BATCH_SIZE):\n",
        "                    state = self.env.reset()\n",
        "                    spatial, non_spatial = self.convert_state(state, 0)\n",
        "                    expert_acts = expert.step(state)\n",
        "                    h = self.encoder(spatial, non_spatial)\n",
        "                    dists, _ = self.ac(h)\n",
        "                    loss = 0.0\n",
        "                    for j in range(self.ac.n_agents):\n",
        "                        loss += -dists[j].log_prob(torch.tensor(expert_acts[j], device=self.device))\n",
        "                    loss = loss.mean()\n",
        "                    self.optimizer.zero_grad()\n",
        "                    loss.backward()\n",
        "                    self.optimizer.step()\n",
        "                    loss_total += loss.item()\n",
        "                self.writer.add_scalar('BC_Loss', loss_total / BC_BATCH_SIZE, epoch)\n",
        "            for param_group in self.optimizer.param_groups:\n",
        "                param_group['lr'] = LR_PPO\n",
        "\n",
        "        for it in tqdm(range(UPDATES)):\n",
        "            self.ent_coef = ENTROPY_COEF_HIGH - (ENTROPY_COEF_HIGH - ENTROPY_COEF_LOW) * (it / UPDATES)\n",
        "            obs, acts, lps, vals, shaped_rews, masks, rews = self.rollout()\n",
        "            total_reward = sum([r.sum().item() for r in rews]) / self.env.n_robots\n",
        "            self.ep_rewards.append(total_reward)\n",
        "            # Ghi reward tổng cho lần lặp này\n",
        "            self.writer.add_scalar('Reward/Total', total_reward, it)\n",
        "\n",
        "            # Ghi reward trung bình qua các episode (tùy chọn)\n",
        "            if len(self.ep_rewards) > 0:\n",
        "                avg_reward = np.mean(self.ep_rewards)\n",
        "                self.writer.add_scalar('Reward/Average', avg_reward, it)\n",
        "\n",
        "            policy_loss, value_loss, entropy = self.update(obs, acts, lps, vals, shaped_rews, masks)\n",
        "            self.writer.add_scalar('Reward', total_reward, it)\n",
        "            self.writer.add_scalar('Policy_Loss', policy_loss, it)\n",
        "            self.writer.add_scalar('Value_Loss', value_loss, it)\n",
        "            self.writer.add_scalar('Entropy', entropy, it)\n",
        "            self.writer.add_scalar('Learning_Rate', self.scheduler.get_last_lr()[0], it)\n",
        "            print(f\"Iteration {it}: Total Reward = {total_reward}, Average Reward = {avg_reward}\")\n",
        "            if total_reward > self.best:\n",
        "                self.best = total_reward\n",
        "                torch.save({\n",
        "                    'encoder': self.encoder.state_dict(),\n",
        "                    'ac': self.ac.state_dict(),\n",
        "                    'optimizer': self.optimizer.state_dict()\n",
        "                }, 'best_model.pth')\n",
        "            self.total_ts += HORIZON\n",
        "            self.scheduler.step()\n",
        "\n",
        "    def evaluate_policy(self, env, num_episodes=10):\n",
        "        total_rewards = []\n",
        "        total_steps = []\n",
        "        for _ in range(num_episodes):\n",
        "            state = env.reset()\n",
        "            done = False\n",
        "            episode_reward = 0\n",
        "            episode_steps = 0\n",
        "            while not done:\n",
        "                spa, non = self.convert_state(state, episode_steps)\n",
        "                h = self.encoder(spa.unsqueeze(0), non.unsqueeze(0))\n",
        "                dists, _ = self.ac(h)\n",
        "                actions = [d.sample().item() for d in dists]\n",
        "                env_actions = [( ['S','L','R','U','D'][a//3], str(a%3) ) for a in actions]\n",
        "                state, reward, done, _ = env.step(env_actions)\n",
        "                episode_reward += sum(reward) if isinstance(reward, (list, np.ndarray)) else reward\n",
        "                episode_steps += 1\n",
        "            total_rewards.append(episode_reward)\n",
        "            total_steps.append(episode_steps)\n",
        "        return np.mean(total_rewards), np.mean(total_steps)\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--map', type=str, default='map2.txt')\n",
        "    parser.add_argument('--num_agents', type=int, default=5)\n",
        "    parser.add_argument('--n_packages', type=int, default=100)\n",
        "    parser.add_argument('--max_steps', type=int, default=1000)\n",
        "    parser.add_argument('--seed', type=int, default=10)\n",
        "    parser.add_argument('--updates', type=int, default=1000)\n",
        "    parser.add_argument('--bc', action='store_true', help='use behavioral cloning pretraining')\n",
        "    parser.add_argument('--save_path', type=str, default='mappo_best.pth')\n",
        "    args, _ = parser.parse_known_args()\n",
        "\n",
        "    set_seed(args.seed)\n",
        "    env = Environment(\n",
        "        map_file=args.map,\n",
        "        n_robots=args.num_agents,\n",
        "        n_packages=args.n_packages,\n",
        "        max_time_steps=args.max_steps,\n",
        "        move_cost=-0.01,\n",
        "        delivery_reward=10.0,\n",
        "        delay_reward=1.0\n",
        "    )\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = MAPPO(env, device, args.max_steps, args)\n",
        "    model.train()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "T7mzXewPAM7v"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "08481d168cdc444a8c744355f7b701da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ee50af5827b040aa8b6b50fe4a2dbd38",
              "IPY_MODEL_2ea2c4b272514f6abd739fec1112510e",
              "IPY_MODEL_5273763364454464b69c6f799798507c"
            ],
            "layout": "IPY_MODEL_a7cda6e563154ccbad32d66cfc55aa5a"
          }
        },
        "ee50af5827b040aa8b6b50fe4a2dbd38": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e3b069e321a74878a2ff033e4d790076",
            "placeholder": "​",
            "style": "IPY_MODEL_1bcca8c264cf47ca8338b76ddfcbe18e",
            "value": "  2%"
          }
        },
        "2ea2c4b272514f6abd739fec1112510e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a0f175f76e9b4f82a821337b1564f527",
            "max": 2000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3b34aa7b622c4bec99a3a9667e740c9d",
            "value": 47
          }
        },
        "5273763364454464b69c6f799798507c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b9545103f47b473da4719bf620301ba7",
            "placeholder": "​",
            "style": "IPY_MODEL_38ce772bd6ec49bd9f2c84153b901f9d",
            "value": " 47/2000 [53:10&lt;36:40:19, 67.60s/it]"
          }
        },
        "a7cda6e563154ccbad32d66cfc55aa5a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e3b069e321a74878a2ff033e4d790076": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1bcca8c264cf47ca8338b76ddfcbe18e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a0f175f76e9b4f82a821337b1564f527": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3b34aa7b622c4bec99a3a9667e740c9d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b9545103f47b473da4719bf620301ba7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "38ce772bd6ec49bd9f2c84153b901f9d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}